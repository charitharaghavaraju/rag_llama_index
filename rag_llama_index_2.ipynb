{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import os\n",
    "import time\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1\", request_timeout=300.0)\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"gemma.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cherry/projects/rag_llama_index/ragenv1/lib/python3.12/site-packages/llama_index/core/evaluation/dataset_generation.py:200: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "/cherry/projects/rag_llama_index/ragenv1/lib/python3.12/site-packages/llama_index/core/evaluation/dataset_generation.py:296: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "eval_documents = documents[:1]\n",
    "data_generator = DatasetGenerator.from_documents(eval_documents, llm=llm)\n",
    "eval_questions = data_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here are 10 questions for the quiz/exam based on the given text:', 'What is the name of the family of lightweight, state-of-the-art open models introduced by Gemma?', 'Which model family was used as inspiration to develop Gemma models?', 'How many sizes of Gemma models are released, and what are their respective parameter counts?', 'In which domains does Gemma achieve strong generalist capabilities in text, alongside state-of-the-art understanding and reasoning skills at scale?', 'What is the total token count used for training Gemma models?', 'Which Google model family was used as a base for developing Gemma?', 'What are the different types of checkpoints released along with the open-source codebase for inference and serving in Gemma?', 'What is the purpose of releasing both pre-trained and fine-tuned checkpoints, according to the authors of Gemma?', 'In which research areas do the authors of Gemma believe the responsible release of LLMs will have a significant impact?', 'Which publication provides more information on the contributions and acknowledgments made in the development of Gemma models?']\n"
     ]
    }
   ],
   "source": [
    "print(eval_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness = FaithfulnessEvaluator()\n",
    "relevancy = RelevancyEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(chunk_size, eval_questions):\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    vector_index = VectorStoreIndex.from_documents(eval_documents)\n",
    "\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        response_vector = query_engine.query(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        faithfulness_result = faithfulness.evaluate_response(response=response_vector).passing\n",
    "        relevancy_result = relevancy.evaluate_response(response=response_vector, query=question).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 128 - Avg. Response Time: 10.10s - Avg. Faithfulness: 0.91 - Avg. Relevancy: 1.00\n",
      "Chunk size: 256 - Avg. Response Time: 13.65s - Avg. Faithfulness: 1.00 - Avg. Relevancy: 1.00\n",
      "Chunk size: 512 - Avg. Response Time: 8.47s - Avg. Faithfulness: 0.91 - Avg. Relevancy: 1.00\n",
      "Chunk size: 1024 - Avg. Response Time: 10.05s - Avg. Faithfulness: 1.00 - Avg. Relevancy: 1.00\n",
      "Chunk size: 2048 - Avg. Response Time: 14.34s - Avg. Faithfulness: 1.00 - Avg. Relevancy: 1.00\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [128, 256, 512, 1024, 2048]\n",
    "\n",
    "for chunk in chunk_sizes:\n",
    "    avg_response_time, avg_faithfulness, avg_relevancy = evaluate(chunk, eval_questions)\n",
    "    print(f\"Chunk size: {chunk} - Avg. Response Time: {avg_response_time:.2f}s - Avg. Faithfulness: {avg_faithfulness:.2f} - Avg. Relevancy: {avg_relevancy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
