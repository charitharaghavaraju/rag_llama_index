{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# GOOGLE_API_KEY = \"AIzaSyAYypmjJtTx0EHUDCEVGPiq79Szcgm5a7Q\"\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting global parameters\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.llm = Gemini(model_name=\"models/gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"llama2.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "# global\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.text_splitter = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[text_splitter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"/cherry/projects/rag_llama_index/research/index_blogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"/cherry/projects/rag_llama_index/research/index_blogs\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a knowledgeable and precise assistant specialized in question-answering tasks, \n",
    "particularly from academic and research-based sources. \n",
    "Your goal is to provide accurate, concise, and contextually relevant answers based on the given information.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Comprehension and Accuracy: Carefully read and comprehend the provided context from the research paper to ensure accuracy in your response.\n",
    "Conciseness: Deliver the answer in no more than three sentences, ensuring it is concise and directly addresses the question.\n",
    "Truthfulness: If the context does not provide enough information to answer the question, clearly state, \"I don't know.\"\n",
    "Contextual Relevance: Ensure your answer is well-supported by the retrieved context and does not include any information beyond what is provided.\n",
    "\n",
    "Remember if no context is provided please say you don't know the answer\n",
    "Here is the question and context for you to work with:\n",
    "\n",
    "\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "prompt_tmplt = PromptTemplate(template=template,\n",
    "                              template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "# Configure retriever\n",
    "retriever = VectorIndexRetriever(index=index,\n",
    "                                 similarity_top_k=10)\n",
    "\n",
    "# Configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# Assemble query engine\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever,\n",
    "                                    response_synthesizer=response_synthesizer)\n",
    "\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": prompt_tmplt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as pretrained and fine-tuned variations.\n"
     ]
    }
   ],
   "source": [
    "## Input\n",
    "response = query_engine.query(\"What are differet variants of LLama?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hyperparameters used for training the model are: AdamW optimizer with β1 = 0.9, β2 = 0.95, eps = 10−5, a cosine learning rate schedule with warmup of 2000 steps, and decay final learning rate down to 10% of the peak learning rate, a weight decay of 0.1, and gradient clipping of 1.0.\n"
     ]
    }
   ],
   "source": [
    "## Input\n",
    "response = query_engine.query(\"What are the hyperparamters used for training the model?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total carbon emissions for training the Llama 2 family of models were estimated to be 539 tCO2eq. 100% of these emissions were offset by Meta's sustainability program.\n"
     ]
    }
   ],
   "source": [
    "## Input\n",
    "response = query_engine.query(\"Can you please comment on the Carbon Footprint of Pretraining. \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
